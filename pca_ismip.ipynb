{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8cffd05-3e3a-44e0-ae5f-babb2791aa86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Margo\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504f7e4-899a-410d-b1bb-5dccda8cc7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory and variables of interest\n",
    "path = \"D:\\\\Thesis_Gl_Projections\"\n",
    "variables = ['lithk', 'acabf']\n",
    "IS = 'GIS'\n",
    "experiment_type = 'ctrl_proj'  # Experiment type you want to match\n",
    "\n",
    "# Initialize a list to hold the matched file paths\n",
    "matched_files = []\n",
    "\n",
    "# Walk through the directory and find matched files\n",
    "for root, dirs, files in os.walk(path):\n",
    "    for filename in files:\n",
    "        # Check if the file matches the required variable patterns, IS, and experiment type\n",
    "        if (any(fnmatch.fnmatch(filename, f\"{var}_*_*_*.nc\") for var in variables) and \n",
    "                fnmatch.fnmatch(filename, f\"*_{IS}*_*_{experiment_type}*.nc\")):\n",
    "            matched_files.append(os.path.join(root, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccf4029-0104-4535-898b-d3c612a9d9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# Initialize a dictionary to hold extracted data for PCA\n",
    "variables = ['acabf', 'lithk']\n",
    "extracted_data = {var: [] for var in variables}\n",
    "\n",
    "# Loop through the matched files and extract data for the year 2015\n",
    "for file in matched_files:\n",
    "    # Extract model and group from the filename\n",
    "    filename_parts = os.path.basename(file).split('_')\n",
    "    group_name = filename_parts[2]  # Assuming group is in the 3rd position of the filename\n",
    "    model_name = filename_parts[3]  # Assuming model is in the 4th position of the filename\n",
    "\n",
    "    with xr.open_dataset(file, engine='netcdf4', use_cftime=True) as ds:\n",
    "        # Extract the data for each variable\n",
    "        for var in variables:\n",
    "            if var in ds.variables:\n",
    "                # Extract data for the variable (keeping all time steps)\n",
    "                var_data = ds[var]  # This is a DataArray with dimensions (time, y, x)\n",
    "\n",
    "                # Extract the data for the year 2015 (assuming it's the first index)\n",
    "                year_index = 0  # Adjust if necessary\n",
    "                year_data = var_data[year_index, :, :]  # Data for the year 2015\n",
    "\n",
    "                # Change zeros to NaN\n",
    "                year_data = year_data.where(year_data != 0, np.nan)  # Replace zeros with NaN\n",
    "\n",
    "                # Convert the DataArray to a DataFrame and reset index\n",
    "                year_df = year_data.to_dataframe().reset_index()\n",
    "\n",
    "                # Drop rows where the variable is NaN\n",
    "                year_df = year_df.dropna(subset=[var])\n",
    "\n",
    "                # Add model and group information to the DataFrame\n",
    "                year_df['model_group'] = model_name + \"_\" + group_name\n",
    "\n",
    "                # Append the cleaned DataFrame for the year 2015 to the extracted data dictionary\n",
    "                extracted_data[var].append(year_df)\n",
    "\n",
    "# Optional: concatenate all DataFrames for each variable if needed\n",
    "for var in extracted_data:\n",
    "    if extracted_data[var]:  # Check if there is any data for the variable\n",
    "        extracted_data[var] = pd.concat(extracted_data[var], ignore_index=True)  # Combine DataFrames\n",
    "    else:\n",
    "        extracted_data[var] = pd.DataFrame()  # Empty DataFrame if no data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb8613e-ae79-43e0-89d2-c01b19ab3d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "\n",
    "# Assuming `extracted_data` is a dictionary containing DataFrames for 'acabf' and 'lithk'\n",
    "acabf_data = extracted_data['acabf']\n",
    "lithk_data = extracted_data['lithk']\n",
    "\n",
    "# Combine the 'acabf' and 'lithk' data into a single DataFrame\n",
    "features = pd.DataFrame({\n",
    "    'acabf': acabf_data['acabf'],\n",
    "    'lithk': lithk_data['lithk'],\n",
    "    'model_group': acabf_data['model_group']\n",
    "})\n",
    "\n",
    "# Check for NaN values and drop them\n",
    "features.dropna(inplace=True)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features[['acabf', 'lithk']])\n",
    "\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "components = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Prepare labels for the explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_ * 100\n",
    "labels = {\n",
    "    'PC1': f\"PC1 ({explained_variance[0]:.1f}%)\",\n",
    "    'PC2': f\"PC2 ({explained_variance[1]:.1f}%)\"\n",
    "}\n",
    "\n",
    "# Create a DataFrame for the PCA components\n",
    "components_df = pd.DataFrame(components, columns=labels.keys())\n",
    "\n",
    "# Add model groups to the DataFrame\n",
    "components_df['model_group'] = features['model_group'].values\n",
    "\n",
    "# Optionally, add a label for each point\n",
    "components_df['label'] = (\n",
    "    features['acabf'].astype(str) + ', ' + features['lithk'].astype(str)\n",
    ")\n",
    "\n",
    "\n",
    "# Create a scatter plot using plotly.express\n",
    "fig = px.scatter_matrix(\n",
    "    components_df,\n",
    "    dimensions=labels.keys(),\n",
    "    color='model_group',\n",
    "    symbol='model_group',\n",
    "    title=\"PCA Scatter Matrix with Model Groups and Ice Thickness + \",\n",
    "    labels=labels,\n",
    "    hover_name='label',\n",
    "    hover_data={\n",
    "        'acabf': True,\n",
    "        'lithk': True\n",
    "    },\n",
    "    opacity=0.6  # Add transparency to handle overlapping points\n",
    ")\n",
    "\n",
    "# Update the plot for better visualization\n",
    "fig.update_traces(diagonal_visible=True)\n",
    "fig.update_layout(showlegend=True)\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
